Ho strutturato il progetto creando una cartella per ogni attore presente nello
schema della soluzione in figura. L'unica modifica che ho fatto all'hwc1 è stata
in realtà un aggiunta: un messaggio di tipo intero per i messaggi che produce il provider.

Il livello di parallelismo sulla lista di reader consentito è 1. Ovvero il dispatcer,
quando ha bisogno della lista di reader a cui mandare il messaggio appena estratto, blocca
con un mutex l'accesso. Si è previsto, come da specifica, che un reader accettato 
successivamente alla presa in carico di un messaggio da parte del dispatcher non deve ricevere
questo messaggio. Di conseguenza finchè il dispatcher non ha inoltrato il messaggio
a tutti i reader presenti al momento del blocco del mutex la lista rimane bloccata.
Sono state valutate altre soluzioni per limitare il prolungarsi della regione critica 
tuttavia per permettere lo scenario descritto appena è stata scelta questa soluzione.

L'accepter blocca la lista dei reader nel momento in cui deve aggiungerne o rimuoverlo uno. La 
responsabilità è stata assegnata a questo elemen.

Inoltre il dispatcher termina un reader lento inviando POISON_PILL dopo che per tre volte
di fila ha trovato il buffer pieno (viene verificato con la put_non_bloccante).
PeCome da requisiti il reader verrà terminato solo quando leggera POISON_PILL, quindi il
dispatcher anche se ha giù inviato POISON_PILL continuerà a provare a spedire messaggi.
Evidentemente questo è un punto che neccesitài una soluzione più efficiente.

E' stata introdotta una struttura dati chiamata list_sync_t che null'altro è che una lista
con un mutex. Utilizzata per gestire i reader.

Sono stati fatti i test unitari per ciascun elemento sviluppato.

Invece il main racchiude uno scenario minimale con un provider che invia 3 messaggi e 
2 reader(veloce, lento).
